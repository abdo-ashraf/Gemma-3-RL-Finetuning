{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":229213030,"sourceType":"kernelVersion"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## Install vllm for fast inference and unsloth for optimized models\n!pip install -qqq unsloth vllm\n!pip install -qqq --upgrade pillow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:07:09.490039Z","iopub.execute_input":"2025-03-24T13:07:09.490246Z","iopub.status.idle":"2025-03-24T13:10:51.787970Z","shell.execute_reply.started":"2025-03-24T13:07:09.490225Z","shell.execute_reply":"2025-03-24T13:10:51.786713Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.5/192.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.3/265.3 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.6/343.6 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.9/126.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.4/376.4 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.6/211.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.7/123.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m913.7/913.7 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nfrom datasets import load_dataset\nfrom trl import GRPOConfig, GRPOTrainer\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:10:51.791734Z","iopub.execute_input":"2025-03-24T13:10:51.792000Z","iopub.status.idle":"2025-03-24T13:11:25.001534Z","shell.execute_reply.started":"2025-03-24T13:10:51.791976Z","shell.execute_reply":"2025-03-24T13:11:25.000880Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth Zoo will now patch everything to make training faster!\nINFO 03-24 13:11:21 [__init__.py:256] Automatically detected platform cuda.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"model_id = \"google/gemma-3-1b-it\"\n\nmax_seq_length = 1024  # Can increase for longer reasoning traces\n# use high ranks (16, 32, 64) for small models (<1B) and small ranks (8, 16) for large models (>1B).\n# Also take memory resource into your considrations.\nlora_r = 8\nlora_dropout = 0\nseed = 3407","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:11:25.002200Z","iopub.execute_input":"2025-03-24T13:11:25.002404Z","iopub.status.idle":"2025-03-24T13:11:25.006203Z","shell.execute_reply.started":"2025-03-24T13:11:25.002386Z","shell.execute_reply":"2025-03-24T13:11:25.005361Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Define the system prompt that instructs the model to use a specific format\nreasoning_start = \"<reasoning>\"\nreasoning_end   = \"</reasoning>\"\nsolution_start = \"<answer>\"\nsolution_end = \"</answer>\"\n\nSYSTEM_PROMPT = \\\nf\"\"\"You are given a problem.\nThink about the problem and provide your working out.\nPlace it between {reasoning_start} and {reasoning_end}.\nThen, provide your solution between {solution_start}{solution_end}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:11:25.007541Z","iopub.execute_input":"2025-03-24T13:11:25.007837Z","iopub.status.idle":"2025-03-24T13:11:25.022708Z","shell.execute_reply.started":"2025-03-24T13:11:25.007810Z","shell.execute_reply":"2025-03-24T13:11:25.022008Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import html\nimport re\nfrom datasets import load_dataset, Dataset\n\n# For GSM8K dataset, We notice all answers like about have a ####, so we extract it\ndef extract_hash_answer(text: str) -> str | None:\n    if \"####\" not in text:\n        return None\n    return text.split(\"####\")[1].strip()\n\n\n# Function to prepare the GSM8K dataset\ndef get_gsm8k_questions(system_prompt:str) -> Dataset:\n    dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n    dataset = dataset.map(\n        lambda x: {\n            \"prompt\": [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": x[\"question\"]},\n            ],\n            \"answer\": extract_hash_answer(x[\"answer\"]),\n        }\n    )\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:11:25.023593Z","iopub.execute_input":"2025-03-24T13:11:25.023912Z","iopub.status.idle":"2025-03-24T13:11:25.035401Z","shell.execute_reply.started":"2025-03-24T13:11:25.023880Z","shell.execute_reply":"2025-03-24T13:11:25.034560Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"dataset = get_gsm8k_questions(system_prompt=SYSTEM_PROMPT)\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:12:17.444846Z","iopub.execute_input":"2025-03-24T13:12:17.445189Z","iopub.status.idle":"2025-03-24T13:12:22.307258Z","shell.execute_reply.started":"2025-03-24T13:12:17.445166Z","shell.execute_reply":"2025-03-24T13:12:22.306534Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5eeca7e463a546a3bedeb5df3406872a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15a5fa9027574873b815a47f2d846e1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2ed4eda78bb4334842252228059c9e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aec5e790112a460e963cf4e9a78d01ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84f812628f3c4554900ca0de5c29b2e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7473 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a22cb8bf0e5b4038a40e81787edbfdec"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['question', 'answer', 'prompt'],\n    num_rows: 7473\n})"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"model, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = model_id,\n    max_seq_length=max_seq_length,\n    load_in_4bit=True,  # Enables memory-efficient training\n    fast_inference=True,  # Enable vLLM fast inference\n    # gpu_memory_utilization=0.6,  # Reduce if out of memory\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:20:52.745601Z","iopub.execute_input":"2025-03-24T13:20:52.745962Z","iopub.status.idle":"2025-03-24T13:21:03.059058Z","shell.execute_reply.started":"2025-03-24T13:20:52.745932Z","shell.execute_reply":"2025-03-24T13:21:03.058346Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.3.18: Fast Gemma3 patching. Transformers: 4.50.0. vLLM: 0.8.1.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: Using float16 precision for gemma3 won't work! Using float32.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"i = 2\nquestion = dataset[i][\"question\"]\n# question = \"Janet pays $40/hour for 3 hours per week of clarinet lessons and $28/hour for 5 hours a week of piano lessons. How much more does she spend on piano lessons than clarinet lessons in a year?\"\n\nprompt = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n          {\"role\": \"user\", \"content\": question}\n          ]\n\n# prompt = [{\"role\": \"user\", \"content\": f\"\\n{SYSTEM_PROMPT}\\n\\n### Question\\n{question}\"}]\n\ntext = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\nprint(text)\nprint(f\"Answer: {dataset[i]['answer']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:21:09.848258Z","iopub.execute_input":"2025-03-24T13:21:09.848555Z","iopub.status.idle":"2025-03-24T13:21:09.855467Z","shell.execute_reply.started":"2025-03-24T13:21:09.848532Z","shell.execute_reply":"2025-03-24T13:21:09.854546Z"}},"outputs":[{"name":"stdout","text":"<bos><start_of_turn>user\nYou are given a problem.\nThink about the problem and provide your working out.\nPlace it between <reasoning> and </reasoning>.\nThen, provide your solution between <answer></answer>\n\nBetty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?<end_of_turn>\n<start_of_turn>model\n\nAnswer: 5\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"model_inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\noutput_ids = model.generate(**model_inputs)\noutput_text = tokenizer.decode(output_ids[0])\nprint(output_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:21:13.027271Z","iopub.execute_input":"2025-03-24T13:21:13.027566Z","iopub.status.idle":"2025-03-24T13:21:16.907661Z","shell.execute_reply.started":"2025-03-24T13:21:13.027544Z","shell.execute_reply":"2025-03-24T13:21:16.906867Z"}},"outputs":[{"name":"stdout","text":"<bos><bos><start_of_turn>user\nYou are given a problem.\nThink about the problem and provide your working out.\nPlace it between <reasoning> and </reasoning>.\nThen, provide your solution between <answer></answer>\n\nBetty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?<end_of_turn>\n<start_of_turn>model\n<answer>100.0</answer><end_of_turn>\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"from peft import PeftModel\n\n# Load the saved LoRA adapter into the base model\nmodel = PeftModel.from_pretrained(model, \"/kaggle/input/finetuning-any-llm/saved_lora_adapter\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:13:40.448880Z","iopub.execute_input":"2025-03-24T13:13:40.449189Z","iopub.status.idle":"2025-03-24T13:13:40.797055Z","shell.execute_reply.started":"2025-03-24T13:13:40.449167Z","shell.execute_reply":"2025-03-24T13:13:40.796204Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"i = 2\nquestion = dataset[i][\"question\"]\n# question = \"Janet pays $40/hour for 3 hours per week of clarinet lessons and $28/hour for 5 hours a week of piano lessons. How much more does she spend on piano lessons than clarinet lessons in a year?\"\n\nprompt = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n          {\"role\": \"user\", \"content\": question}\n          ]\n\n# prompt = [{\"role\": \"user\", \"content\": f\"\\n{SYSTEM_PROMPT}\\n\\n### Question\\n{question}\"}]\n\ntext = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\nprint(text)\nprint(f\"Answer: {dataset[i]['answer']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:20:21.909901Z","iopub.execute_input":"2025-03-24T13:20:21.910359Z","iopub.status.idle":"2025-03-24T13:20:21.917822Z","shell.execute_reply.started":"2025-03-24T13:20:21.910321Z","shell.execute_reply":"2025-03-24T13:20:21.916835Z"}},"outputs":[{"name":"stdout","text":"<bos><start_of_turn>user\nYou are given a problem.\nThink about the problem and provide your working out.\nPlace it between <reasoning> and </reasoning>.\nThen, provide your solution between <answer></answer>\n\nBetty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?<end_of_turn>\n<start_of_turn>model\n\nAnswer: 5\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"model_inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\noutput_ids = model.generate(**model_inputs)\noutput_text = tokenizer.decode(output_ids[0])\nprint(output_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:18:58.096230Z","iopub.execute_input":"2025-03-24T13:18:58.096566Z","iopub.status.idle":"2025-03-24T13:19:21.180605Z","shell.execute_reply.started":"2025-03-24T13:18:58.096535Z","shell.execute_reply":"2025-03-24T13:19:21.179697Z"}},"outputs":[{"name":"stdout","text":"<bos><bos><start_of_turn>user\nYou are given a problem.\nThink about the problem and provide your working out.\nPlace it between <reasoning> and </reasoning>.\nThen, provide your solution between <answer></answer>\n\nBetty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?<end_of_turn>\n<start_of_turn>model\n<reasoning>\nLet the cost of the wallet be $100.\nBetty has half of the money she needs, so she has $\\frac{1}{2} \\times 100 = 50$.\nHer parents gave her $15.\nHer grandparents gave her twice as much as her parents, so grandparents gave her $2 \\times 15 = 30$.\nThe total amount of money her grandparents gave her is $30$.\nSo, the total amount of money Betty has is $50 + 15 + 30 = 95$.\nThe amount of money she needs is $100 - 95 = 5$.\nTherefore, Betty needs $5 more to buy the wallet.\n</reasoning>\n<answer>5</answer><end_of_turn>\n","output_type":"stream"}],"execution_count":23}]}